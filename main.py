import os
import sys
import asyncio
import time
import api_keys
import json
import uuid
import tempfile

# Set up path for importing YARS
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
src_path = os.path.join(project_root, "src")
sys.path.append(src_path)

from yars.yars import YARS
from google_sheets_utils import GoogleSheetsClient
from gemini import GeminiClient

# Initialize the YARS Reddit miner
miner = YARS()

# Initialize the Google Sheets client
sheets_client = GoogleSheetsClient()

# Initialize the Gemini client
gemini_client = GeminiClient()

# Main async function to scrape subreddit data
async def scrape_subreddit_data_async(subreddit_name, category, limit=5, filter=None):
    try:
        # Check if we already have 50 or more rows in the sheet
        row_count = sheets_client.get_row_count(api_keys.GOOGLE_SHEET_NAMES["REDDIT_DATA"])
        if row_count >= 50:
            print(f"Sheet already has {row_count} rows, skipping Reddit scraping")
            return
        
        print(f"Sheet has {row_count} rows, proceeding with Reddit scraping")
        
        # Fetch posts from Reddit
        subreddit_posts = miner.fetch_subreddit_posts(subreddit_name, category, limit=limit, time_filter="all", filter=filter)
        
        # Create tasks for processing each post
        tasks = []
        for i, post in enumerate(subreddit_posts, 1):
            print(f"Creating task for post {i}")
            tasks.append(sheets_client.process_post(post, miner))
        
        # Wait for all tasks to complete
        results = await asyncio.gather(*tasks)
        
        # Filter out None results and duplicates
        non_duplicate_posts = []
        for post_data in results:
            if post_data and not sheets_client.is_duplicate_post(post_data.get("title", ""), post_data.get("author", "")):
                non_duplicate_posts.append(post_data)
            elif post_data:
                print(f"Skipping duplicate post: '{post_data.get('title', '')}'")
        
        if not non_duplicate_posts:
            print("No new posts to process after filtering duplicates")
            return
            
        # Create a temporary Python file with all non-duplicate posts
        temp_file_path = create_temp_python_file(non_duplicate_posts)
        
        try:
            # Analyze all posts with a single Gemini API call, passing the temp file path
            analysis_results = gemini_client.analyze_reddit_posts_batch(temp_file_path)
            print(analysis_results)
            
            # Process the analysis results and add to Google Sheets
            successful_posts = 0
            for i, post_data in enumerate(non_duplicate_posts):
                if i < len(analysis_results):
                    analysis_result = analysis_results[i]
                    post_data["analysis"] = analysis_result
                    
                    # Only add to sheet if the post should be processed
                    if analysis_result.get("should_process", False):
                        if sheets_client.add_to_sheet(post_data, api_keys.GOOGLE_SHEET_NAMES["REDDIT_DATA"], skip_duplicate_check=True):
                            successful_posts += 1
                            print(f"Added post: '{post_data.get('title', '')}' (Quality: {analysis_result.get('quality_rating', 0)})")
                    else:
                        print(f"Skipping post: '{post_data.get('title', '')}' - Not a relevant finance/business question")
            
            print(f"Successfully added {successful_posts} posts to Google Sheets")
        finally:
            # Clean up the temporary file
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
                print(f"Removed temporary file: {temp_file_path}")
        
    except Exception as e:
        print(f"Error occurred while scraping subreddit: {e}")
        import traceback
        traceback.print_exc()

def create_temp_python_file(post_data_list):
    """
    Create a temporary Python file with a random name containing the post data
    
    Args:
        post_data_list (list): List of post data to save
        
    Returns:
        str: Path to the created temporary file
    """
    # Generate a random filename
    random_filename = f"temp_posts_{uuid.uuid4().hex}.py"
    file_path = os.path.join(tempfile.gettempdir(), random_filename)
    
    # Format the post data as a Python variable
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write("# Temporary file containing Reddit post data\n\n")
        f.write("# This file is automatically generated and will be deleted after processing\n\n")
        f.write("# Post data in Python format\n")
        f.write("reddit_posts = ")
        f.write(repr(post_data_list))
        f.write("\n")
    
    print(f"Created temporary Python file with {len(post_data_list)} posts: {file_path}")
    return file_path

def generate_article_for_first_post():
    """
    Fetch the first row from the Google Sheet (excluding headers) and generate
    a detailed, up-to-date article about the question using Gemini with web search.
    
    Returns:
        tuple: (post_data, article_content) if successful, (None, None) otherwise
    """
    try:
        # Get the first row from the sheet (excluding headers)
        first_post = sheets_client.get_first_unprocessed_post(api_keys.GOOGLE_SHEET_NAMES["REDDIT_DATA"])
        
        if not first_post:
            print("No unprocessed posts found in the sheet")
            return None, None
        
        # Validate that the post has a title and body
        if not first_post.get('Title', '').strip():
            print("Post has no title content")
            return None, None
            
        if not first_post.get('Body', '').strip():
            print("Post has no body content")
            return None, None
        print(f"Generating article for post: '{first_post.get('Title', '')}'")
        
        # print(f"Generating article for post: '{first_post.get('Title', '')}' with body length: {len(first_post.get('Body', ''))}")
        
        # Construct a prompt for Gemini that includes the question and instructions
        prompt = f"""
        Generate a comprehensive, detailed response to this finance/business question using the most current information available.
        
        QUESTION TITLE: {first_post.get('Title', '')}
        
        QUESTION DETAILS: {first_post.get('Body', '')}
        
        IMPORTANT INSTRUCTIONS:
        1. Use the most up-to-date information available as of today
        2. Provide factual, accurate information with specific details
        3. Include relevant statistics, examples, and expert opinions when applicable
        4. Structure the response in a clear, organized manner with appropriate headings
        5. Do NOT include any introduction or conclusion sections
        6. Start directly with the main content
        7. Focus on providing practical, actionable advice when appropriate
        8. Cite specific sources or references when possible
        
        Your response should be thorough and comprehensive, addressing all aspects of the question.
        """
        
        # Generate the article using Gemini with web search enabled
        article_content = gemini_client.generate_text_with_web_search(prompt)
        
        if not article_content:
            print("Failed to generate article content")
            return None, None
        
        # Return the post data and article content for further processing
        return first_post, article_content
            
    except Exception as e:
        print(f"Error generating article: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def generate_image_for_article(post_data, article_content):
    """
    Generate an image for the article using a multi-stage process:
    1. Generate image prompt using Puter API (Claude)
    2. If that fails, use GitHub API (GPT-4o)
    3. If that fails, use Gemini to generate the prompt
    4. Generate image using Together API
    5. If that fails, use Gemini for image generation
    6. Upload image to Google Drive and get public URL
    7. Delete local image file after upload
    
    Args:
        post_data (dict): The post data from Google Sheets
        article_content (str): The generated article content
        
    Returns:
        dict: Image generation result with image data, prompt, alt tag, and public URL
    """
    try:
        print("Starting image generation process...")
        
        # Create a single prompt message that will be used for all API attempts
        claude_messages = [
            {"role": "user", "content": f"""
            Based on the following article about a finance/business question, create:
            1. A detailed image prompt that would generate a relevant, professional illustration or visualization
            2. A concise alt tag (25-40 words) that describes the image for accessibility purposes
            
            The image should be suitable for a business blog and should visually represent the key concepts 
            in the article. Make the prompt detailed and specific, focusing on creating a professional-looking 
            image that enhances the article's content.
            
            ARTICLE TITLE: {post_data.get('Title', '')}
            
            ARTICLE CONTENT: {article_content}
            
            Provide your response in JSON format with two fields:
            {{
                "image_prompt": "your detailed image generation prompt here",
                "alt_tag": "your concise alt tag description here"
            }}
            
            Include ONLY the JSON with no additional text or explanations.
            For the image_prompt, include this negative prompt: (blurry, low-resolution, pixelated textures, compression artifacts, watermark, text, logo, poor anatomy, extra limbs, fused body parts, misplaced facial features, disproportionate body, stiff or awkward pose, unnatural joint angles, distorted hands or faces, bad composition, flat lighting, overexposed or underexposed areas, washed-out colors, overly saturated tones, muddy shadows, plastic or waxy textures, cartoonish style (unless specified), simplistic or childish look, symmetry glitches, duplicated limbs or faces, noisy background, pattern repetition, incorrect perspective, unrealistic materials, unfinished render, amateurish style)
            """}
        ]
        
        # Step 1: Generate image prompt and alt tag using Puter API
        print("Generating image prompt and alt tag using Puter API...")
        from puter_api import ChatCompletion
        import json
        
        try:
            # Try using Claude via Puter API
            claude_response = ChatCompletion.create(
                messages=claude_messages
            )
            # The updated ChatCompletion.create() will handle model selection and API key rotation
            
            # Handle the response based on whether it's a success or error
            if isinstance(claude_response, dict) and "error" in claude_response:
                # If we got an error response, raise an exception to trigger the GitHub fallback
                raise Exception(f"Error from ChatCompletion: {claude_response['error']}")
            
            # Parse the response - the format depends on whether Claude or GPT-4o was used
            if isinstance(claude_response, str):
                # Direct text response from Claude
                response_text = claude_response
            else:
                # Response object from GPT-4o
                response_text = claude_response['result']['message']['content']
                
            # Parse the JSON response
            try:
                image_data = json.loads(response_text)
                image_prompt = image_data.get("image_prompt", "")
                alt_tag = image_data.get("alt_tag", "")
                print(f"Successfully generated image prompt using Puter API: {image_prompt[:100]}...")
                print(f"Alt tag: {alt_tag}")
            except json.JSONDecodeError:
                # If JSON parsing fails, use the text as image prompt and generate a generic alt tag
                image_prompt = response_text
                alt_tag = f"Image related to {post_data.get('Title', '')}"  
                print("Failed to parse JSON response from Puter API, using text as image prompt")
                
        except Exception as claude_error:
            print(f"Error using Puter API for image prompt: {claude_error}")
            
            # Step 2: Fall back to GitHub API
            print("Falling back to GitHub API for image prompt and alt tag generation...")
            from github_api import GitHubCompletion
            
            try:
                # Try using GitHub API with the same messages
                github_response = GitHubCompletion.create(claude_messages)
                
                # Handle the response based on whether it's a success or error
                if isinstance(github_response, dict) and "error" in github_response:
                    # If we got an error response, raise an exception to trigger the Gemini fallback
                    raise Exception(f"Error from GitHubCompletion: {github_response['error']}")
                
                # Parse the JSON response
                try:
                    # First attempt: direct JSON parsing
                    try:
                        image_data = json.loads(github_response)
                        image_prompt = image_data.get("image_prompt", "")
                        alt_tag = image_data.get("alt_tag", "")
                    except json.JSONDecodeError:
                        # Second attempt: Try to extract JSON from text (in case there's additional text)
                        import re
                        json_match = re.search(r'\{[\s\S]*\}', github_response)
                        if json_match:
                            try:
                                json_str = json_match.group(0)
                                image_data = json.loads(json_str)
                                image_prompt = image_data.get("image_prompt", "")
                                alt_tag = image_data.get("alt_tag", "")
                            except json.JSONDecodeError:
                                raise  # Re-raise to be caught by outer exception handler
                        else:
                            raise json.JSONDecodeError("No JSON object found in response", github_response, 0)
                    
                    # Validate that we have meaningful content
                    if not image_prompt.strip():
                        raise ValueError("Empty image prompt in parsed JSON")
                        
                    print(f"Successfully generated image prompt using GitHub API: {image_prompt[:100]}...")
                    print(f"Alt tag: {alt_tag}")
                except (json.JSONDecodeError, ValueError) as json_error:
                    # If JSON parsing fails, use the text as image prompt and generate a generic alt tag
                    print(f"Failed to parse JSON response from GitHub API: {json_error}")
                    image_prompt = github_response
                    alt_tag = f"Image related to {post_data.get('Title', '')}"  
                    print("Using text as image prompt instead")
                    
            except Exception as github_error:
                print(f"Error using GitHub API for image prompt: {github_error}")
                
                # Step 3: Fall back to Gemini for image prompt and alt tag
                print("Falling back to Gemini for image prompt and alt tag generation...")
                
                # Use Gemini to generate the image prompt and alt tag with the same prompt content
                gemini_prompt = claude_messages[0]["content"]
                gemini_response = gemini_client.generate_text(gemini_prompt)
                
                # Parse the JSON response
                try:
                    # First attempt: direct JSON parsing
                    try:
                        image_data = json.loads(gemini_response)
                        image_prompt = image_data.get("image_prompt", "")
                        alt_tag = image_data.get("alt_tag", "")
                    except json.JSONDecodeError:
                        # Second attempt: Try to extract JSON from text (in case there's additional text)
                        import re
                        json_match = re.search(r'\{[\s\S]*\}', gemini_response)
                        if json_match:
                            try:
                                json_str = json_match.group(0)
                                image_data = json.loads(json_str)
                                image_prompt = image_data.get("image_prompt", "")
                                alt_tag = image_data.get("alt_tag", "")
                            except json.JSONDecodeError:
                                raise  # Re-raise to be caught by outer exception handler
                        else:
                            raise json.JSONDecodeError("No JSON object found in response", gemini_response, 0)
                    
                    # Validate that we have meaningful content
                    if not image_prompt.strip():
                        raise ValueError("Empty image prompt in parsed JSON")
                        
                    print(f"Successfully generated image prompt using Gemini: {image_prompt[:100]}...")
                    print(f"Alt tag: {alt_tag}")
                except (json.JSONDecodeError, ValueError) as json_error:
                    # If JSON parsing fails, use the text as image prompt and generate a generic alt tag
                    print(f"Failed to parse JSON response from Gemini: {json_error}")
                    image_prompt = gemini_response
                    alt_tag = f"Image related to {post_data.get('Title', '')}"  
                    print("Using text as image prompt instead")
        
        # Step 4: Generate the image using Together API via ImageGeneration class
        # print("Generating image using Together API...")
        # from image_generation import ImageGeneration
        # image_generator = ImageGeneration()
        
        # # Generate a filename based on the post title
        # import re
        # safe_title = re.sub(r'[^\w\s-]', '', post_data.get('Title', 'article_image')).strip().replace(' ', '_')
        # image_path = f"{safe_title}_image.png"
        
        # try:
        #     # Try using Together API for image generation
        #     image_result = image_generator.generate_image(
        #         prompt=image_prompt,
        #         width=1280,
        #         height=720,
        #         steps=4,
        #         n=1,
        #         save_path=image_path
        #     )
            
        #     print(f"Successfully generated image using Together API: {image_path}")
            
        #     # Step 5: Upscale the generated image using upload_image.py
        #     print("Upscaling the generated image...")
        #     from upload_image import process_image
            
        #     # Process the image with 2x upscaling
        #     upscale_result = process_image(
        #         image_path=image_path,
        #         scale_factor="2x",
        #         save_result=True,
        #         output_dir="processed_images"
        #     )
            
        #     if upscale_result.get("status") == "success" and upscale_result.get("local_path"):
        #         upscaled_image_path = upscale_result.get("local_path")
        #         print(f"Successfully upscaled image: {upscaled_image_path}")
        #     else:
        #         print(f"Upscaling failed, using original image: {image_path}")
        #         upscaled_image_path = image_path
            
        # except Exception as together_error:
        #     print(f"Error using Together API for image generation: {together_error}")
            
        #     # Step 6: Fall back to Gemini for image generation (if implemented)
        #     print("Falling back to Gemini for image generation...")
        #     # Note: This would require implementing Gemini image generation
        #     # For now, we'll raise an error
        #     raise Exception("Together API failed and Gemini image generation is not implemented")
        
        # # Step 7: Upload the upscaled image to Google Drive and get public URL
        # print("Uploading upscaled image to Google Drive...")

        # try:
        #     from google_drive_utils import GoogleDriveClient
            
        #     drive_client = GoogleDriveClient()
            
        #     # Upload the upscaled image to Google Drive
        #     file_metadata = drive_client.upload_image(
        #         image_path=upscaled_image_path,
        #         folder_id=api_keys.GOOGLE_DRIVE_FOLDER_ID
        #     )
            
        #     # Get the public URL
        #     public_url = file_metadata.get('webContentLink') or file_metadata.get('webViewLink')
        #     print(f"Upscaled image uploaded to Google Drive: {public_url}")
            
        #     # Step 8: Delete both the original and upscaled image files after successful upload
        #     if os.path.exists(image_path):
        #         os.remove(image_path)
        #         print(f"Deleted original image file: {image_path}")
                
        #     if os.path.exists(upscaled_image_path) and upscaled_image_path != image_path:
        #         os.remove(upscaled_image_path)
        #         print(f"Deleted upscaled image file: {upscaled_image_path}")
            
        #     return {
        #         "success": True,
        #         "image_prompt": image_prompt,
        #         "alt_tag": alt_tag,
        #         "public_url": public_url,
        #         "file_id": file_metadata.get('id')
        #     }
        # except Exception as e:
        #     print(f"Error in saving image in google drive {e}")
    
                
    except Exception as e:
        print(f"Error in image generation process: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

# Function to run the async scraper
def run_async_scraper(subreddit_name, category="new", limit=5, filter=None):
    start_time = time.time()
    asyncio.run(scrape_subreddit_data_async(subreddit_name, category, limit, filter))
    end_time = time.time()
    print(f"Total execution time: {end_time - start_time:.2f} seconds")

# Main execution
if __name__ == "__main__":
    # Subreddit to scrape
    # subreddit_name = "https://www.reddit.com/user/old_temperature4590/m/financebusiness"
    
    # # Example of using filter
    # filter_flairs = ["Question", "Help"]
    
    # # Run the async scraper
    # run_async_scraper(
    #     subreddit_name=subreddit_name,
    #     category="new",
    #     limit=5,
    #     filter=filter_flairs
    # )
    
    # After scraping is complete, generate an article for the first post
    # but don't save it to the sheet yet
    post_data, article_content = generate_article_for_first_post()
    
    if post_data and article_content:
        print("Article generation successful!")
        print(f"Post title: {post_data.get('Title', '')}")
        print(f"Article preview: {article_content[:200]}...")

        # Generate an image for the article
        image_result = generate_image_for_article(post_data, article_content)
        
        if image_result.get("success", True):
            print(f"Image generation successful! Saved to: {image_result.get('image_path')}")
        else:
            print(f"Image generation failed: {image_result.get('error', 'Unknown error')}")
    # else:
    #     print("No article was generated")
