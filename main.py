import os
import sys
import asyncio
import time
import api_keys
import json
import uuid
import tempfile

# Set up path for importing YARS
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
src_path = os.path.join(project_root, "src")
sys.path.append(src_path)

from yars.yars import YARS
from google_sheets_utils import GoogleSheetsClient
from gemini import GeminiClient

# Initialize the YARS Reddit miner
miner = YARS()

# Initialize the Google Sheets client
sheets_client = GoogleSheetsClient()

# Initialize the Gemini client
gemini_client = GeminiClient()

# Main async function to scrape subreddit data
async def scrape_subreddit_data_async(subreddit_name, category, limit=5, filter=None):
    try:
        # Check if we already have 50 or more rows in the sheet
        row_count = sheets_client.get_row_count(api_keys.GOOGLE_SHEET_NAMES["REDDIT_DATA"])
        if row_count >= 50:
            print(f"Sheet already has {row_count} rows, skipping Reddit scraping")
            return
        
        print(f"Sheet has {row_count} rows, proceeding with Reddit scraping")
        
        # Fetch posts from Reddit
        subreddit_posts = miner.fetch_subreddit_posts(subreddit_name, category, limit=limit, time_filter="all", filter=filter)
        
        # Create tasks for processing each post
        tasks = []
        for i, post in enumerate(subreddit_posts, 1):
            print(f"Creating task for post {i}")
            tasks.append(sheets_client.process_post(post, miner))
        
        # Wait for all tasks to complete
        results = await asyncio.gather(*tasks)
        
        # Filter out None results and duplicates
        non_duplicate_posts = []
        for post_data in results:
            if post_data and not sheets_client.is_duplicate_post(post_data.get("title", ""), post_data.get("author", "")):
                non_duplicate_posts.append(post_data)
            elif post_data:
                print(f"Skipping duplicate post: '{post_data.get('title', '')}'")
        
        if not non_duplicate_posts:
            print("No new posts to process after filtering duplicates")
            return
            
        # Create a temporary Python file with all non-duplicate posts
        temp_file_path = create_temp_python_file(non_duplicate_posts)
        
        try:
            # Analyze all posts with a single Gemini API call, passing the temp file path
            analysis_results = gemini_client.analyze_reddit_posts_batch(temp_file_path)
            print(analysis_results)
            
            # Process the analysis results and add to Google Sheets
            successful_posts = 0
            for i, post_data in enumerate(non_duplicate_posts):
                if i < len(analysis_results):
                    analysis_result = analysis_results[i]
                    post_data["analysis"] = analysis_result
                    
                    # Only add to sheet if the post should be processed
                    if analysis_result.get("should_process", False):
                        if sheets_client.add_to_sheet(post_data, api_keys.GOOGLE_SHEET_NAMES["REDDIT_DATA"], skip_duplicate_check=True):
                            successful_posts += 1
                            print(f"Added post: '{post_data.get('title', '')}' (Quality: {analysis_result.get('quality_rating', 0)})")
                    else:
                        print(f"Skipping post: '{post_data.get('title', '')}' - Not a relevant finance/business question")
            
            print(f"Successfully added {successful_posts} posts to Google Sheets")
        finally:
            # Clean up the temporary file
            if os.path.exists(temp_file_path):
                os.remove(temp_file_path)
                print(f"Removed temporary file: {temp_file_path}")
        
    except Exception as e:
        print(f"Error occurred while scraping subreddit: {e}")
        import traceback
        traceback.print_exc()

def create_temp_python_file(post_data_list):
    """
    Create a temporary Python file with a random name containing the post data
    
    Args:
        post_data_list (list): List of post data to save
        
    Returns:
        str: Path to the created temporary file
    """
    # Generate a random filename
    random_filename = f"temp_posts_{uuid.uuid4().hex}.py"
    file_path = os.path.join(tempfile.gettempdir(), random_filename)
    
    # Format the post data as a Python variable
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write("# Temporary file containing Reddit post data\n\n")
        f.write("# This file is automatically generated and will be deleted after processing\n\n")
        f.write("# Post data in Python format\n")
        f.write("reddit_posts = ")
        f.write(repr(post_data_list))
        f.write("\n")
    
    print(f"Created temporary Python file with {len(post_data_list)} posts: {file_path}")
    return file_path

def generate_article_for_first_post():
    """
    Fetch the first row from the Google Sheet (excluding headers) and generate
    a detailed, up-to-date article about the question using Gemini with web search.
    
    Returns:
        tuple: (post_data, article_content) if successful, (None, None) otherwise
    """
    try:
        # Get the first row from the sheet (excluding headers)
        first_post = sheets_client.get_first_unprocessed_post(api_keys.GOOGLE_SHEET_NAMES["REDDIT_DATA"])
        
        if not first_post:
            print("No unprocessed posts found in the sheet")
            return None, None
        
        print(f"Generating article for post: '{first_post.get('title', '')}'")
        
        # Construct a prompt for Gemini that includes the question and instructions
        prompt = f"""
        Generate a comprehensive, detailed response to this finance/business question using the most current information available.
        
        QUESTION TITLE: {first_post.get('title', '')}
        
        QUESTION DETAILS: {first_post.get('body', '')}
        
        IMPORTANT INSTRUCTIONS:
        1. Use the most up-to-date information available as of today
        2. Provide factual, accurate information with specific details
        3. Include relevant statistics, examples, and expert opinions when applicable
        4. Structure the response in a clear, organized manner with appropriate headings
        5. Do NOT include any introduction or conclusion sections
        6. Start directly with the main content
        7. Focus on providing practical, actionable advice when appropriate
        8. Cite specific sources or references when possible
        
        Your response should be thorough and comprehensive, addressing all aspects of the question.
        """
        
        # Generate the article using Gemini with web search enabled
        article_content = gemini_client.generate_text_with_web_search(prompt)
        
        if not article_content:
            print("Failed to generate article content")
            return None, None
        
        # Return the post data and article content for further processing
        return first_post, article_content
            
    except Exception as e:
        print(f"Error generating article: {e}")
        import traceback
        traceback.print_exc()
        return None, None

# Function to run the async scraper
def run_async_scraper(subreddit_name, category="new", limit=5, filter=None):
    start_time = time.time()
    asyncio.run(scrape_subreddit_data_async(subreddit_name, category, limit, filter))
    end_time = time.time()
    print(f"Total execution time: {end_time - start_time:.2f} seconds")

# Main execution
if __name__ == "__main__":
    # Subreddit to scrape
    # subreddit_name = "https://www.reddit.com/user/old_temperature4590/m/financebusiness"
    
    # # Example of using filter
    # filter_flairs = ["Question", "Help"]
    
    # # Run the async scraper
    # run_async_scraper(
    #     subreddit_name=subreddit_name,
    #     category="new",
    #     limit=5,
    #     filter=filter_flairs
    # )
    
    # After scraping is complete, generate an article for the first post
    # but don't save it to the sheet yet
    post_data, article_content = generate_article_for_first_post()
    
    if post_data and article_content:
        print("Article generation successful!")
        print(f"Post title: {post_data.get('title', '')}")
        print(f"Article preview: {article_content[:200]}...")
        
        # Here you can add additional processing steps for the article
        # For example:
        # 1. Format the article
        # 2. Add images
        # 3. Add SEO metadata
        # 4. Generate social media snippets
        # 5. etc.
        
        # When ready to save (commented out for now):
        # if sheets_client.mark_post_as_done(
        #     post_title=post_data.get('title', ''),
        #     article_content=article_content,
        #     status="Completed"
        # ):
        #     print(f"Successfully saved article for: '{post_data.get('title', '')}'")
        # else:
        #     print(f"Failed to save article to sheet")
    else:
        print("No article was generated")